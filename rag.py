import os
import json
from pathlib import Path
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.docstore.document import Document

# =====================================
# 1ï¸âƒ£ å‚æ•°è®¾ç½®
# =====================================
EMBED_MODEL = "BAAI/bge-large-zh"
CHUNKS_PATH = "longzu13.jsonl"  # ä¹‹å‰ç”Ÿæˆçš„ JSONL
INDEX_DIR = "longzu13_index"  # ä¿å­˜ç´¢å¼•çš„ç›®å½•

# =====================================
# 2ï¸âƒ£ åˆå§‹åŒ–å‘é‡æ¨¡å‹ï¼ˆâœ… å¯ç”¨å½’ä¸€åŒ–ï¼‰
# =====================================
print("ğŸš€ æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹:", EMBED_MODEL)
embeddings = HuggingFaceEmbeddings(
    model_name=EMBED_MODEL,
    encode_kwargs={"normalize_embeddings": True}  # âœ… å¯ç”¨L2å½’ä¸€åŒ– => å†…ç§¯â‰ˆä½™å¼¦ç›¸ä¼¼åº¦
)

# =====================================
# 3ï¸âƒ£ åŠ è½½åˆ‡åˆ†åçš„æ•°æ®
# =====================================
print("ğŸ“– è¯»å– JSONL æ–‡ä»¶:", CHUNKS_PATH)
docs = []
with open(CHUNKS_PATH, "r", encoding="utf-8") as f:
    for line in f:
        item = json.loads(line)
        if not item.get("text"):
            continue

        metadata = {
            "chunk_index": int(item.get("chunk_index", -1)),
            "chapter_index": int(item.get("chapter_index", -1)),   # âœ… æ”¹æˆæ•°å­—ç´¢å¼•
            "offset_in_chapter": int(item.get("offset_in_chapter", -1)),
        }
        docs.append(Document(page_content=item["text"], metadata=metadata))

print(f"âœ… åŠ è½½å®Œæˆï¼Œå…± {len(docs)} ä¸ªæ–‡æœ¬å—")

# =====================================
# 4ï¸âƒ£ åˆ›å»º FAISS å‘é‡ç´¢å¼•ï¼ˆä½¿ç”¨å†…ç§¯ç›¸ä¼¼åº¦ï¼‰
# =====================================
print("ğŸ” æ­£åœ¨æ„å»º FAISS å‘é‡ç´¢å¼•ï¼ˆå·²å½’ä¸€åŒ–ï¼‰...")
vectorstore = FAISS.from_documents(docs, embeddings)

# =====================================
# 5ï¸âƒ£ ä¿å­˜ç´¢å¼•
# =====================================
vectorstore.save_local(INDEX_DIR)
print(f"ğŸ’¾ ç´¢å¼•å·²ä¿å­˜åˆ°: {INDEX_DIR}")

# =====================================
# 6ï¸âƒ£ éªŒè¯åŠ è½½
# =====================================
print("ğŸ” æµ‹è¯•åŠ è½½ç´¢å¼•å¹¶æ£€ç´¢ç¤ºä¾‹...")
loaded = FAISS.load_local(INDEX_DIR, embeddings, allow_dangerous_deserialization=True)
retriever = loaded.as_retriever(search_kwargs={"k": 3})

query = "ç»˜æ¢¨è¡£ä¹‹æ­»"
results = retriever.get_relevant_documents(query)
for i, r in enumerate(results):
    print(f"\n--- Top {i+1} ---")
    print(f"ç« èŠ‚ç´¢å¼•: {r.metadata.get('chapter_index')}")
    print(f"ç« èŠ‚å†…åç§»: {r.metadata.get('offset_in_chapter')}")
    print(f"å…¨å±€chunkç´¢å¼•: {r.metadata.get('chunk_index')}")
    print(f"å†…å®¹é¢„è§ˆ: {r.page_content[:]}")
